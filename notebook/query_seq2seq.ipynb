{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd9fe7-e6a8-4482-9464-f2f21ed2c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = field(\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\n",
    "            \"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"\n",
    "        },\n",
    "    )\n",
    "    model_code_revision: str = field(\n",
    "        default=None, metadata={\"help\": \"The branch of the IFT model\"}\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The path to the tokenizer. Useful if you want to use a different tokenizer to the one stored in `model_name_or_path`.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False, metadata={\"help\": \"Trust remote code when loading a model.\"}\n",
    "    )\n",
    "    use_flash_attention_2: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to use flash attention 2. You must install this manually by running `pip install flash-attn --no-build-isolation`\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    use_peft: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": (\"Whether to use PEFT or not for training.\")},\n",
    "    )\n",
    "    lora_r: Optional[int] = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": (\"LoRA R value.\")},\n",
    "    )\n",
    "    lora_alpha: Optional[int] = field(\n",
    "        default=32,\n",
    "        metadata={\"help\": (\"LoRA alpha.\")},\n",
    "    )\n",
    "    lora_dropout: Optional[float] = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": (\"LoRA dropout.\")},\n",
    "    )\n",
    "    lora_target_modules: Optional[List[str]] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"LoRA target modules.\")},\n",
    "    )\n",
    "    lora_modules_to_save: Optional[List[str]] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"Model layers to unfreeze & train\")},\n",
    "    )\n",
    "    load_in_8bit: bool = field(default=False, metadata={\"help\": \"use 8 bit precision\"})\n",
    "    load_in_4bit: bool = field(default=False, metadata={\"help\": \"use 4 bit precision\"})\n",
    "\n",
    "    bnb_4bit_quant_type: Optional[str] = field(\n",
    "        default=\"nf4\", metadata={\"help\": \"precise the quantization type (fp4 or nf4)\"}\n",
    "    )\n",
    "    use_bnb_nested_quant: bool = field(\n",
    "        default=False, metadata={\"help\": \"use nested quantization\"}\n",
    "    )\n",
    "    bnb_4bit_quant_storage: Optional[str] = field(\n",
    "        default=\"uint8\",\n",
    "        metadata={\"help\": \"storage type to pack the quanitzed 4-bit prarams.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.load_in_8bit and self.load_in_4bit:\n",
    "            raise ValueError(\"You can't use 8 bit and 4 bit precision at the same time\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_template: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The chat template to use.\"}\n",
    "    )\n",
    "\n",
    "    text_column: Optional[str] = field(\n",
    "        default=\"text\",\n",
    "        metadata={\n",
    "            \"help\": \"The column name to use for the text in the dataset (only used for continued pretraining).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    truncation_side: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Truncation side to use for the tokenizer.\"}\n",
    "    )\n",
    "    auto_insert_empty_system_msg: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to automatically insert an empty system message as the first message if `system` is mentioned in the chat template.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    train_dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The path to the training dataset.\")},\n",
    "    )\n",
    "    test_dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The path to the training dataset.\")},\n",
    "    )    \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig:\n",
    "    \"\"\"\n",
    "    Arguments related to the training process itself. For all parameters, see: https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    Also used for the continued pretraining task.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_kwargs: Optional[Dict[str, Any]] = field(\n",
    "        default=None, metadata={\"help\": \"Dataset kwargs for the SFTTrainer\"}\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=8196,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Used by TRL for reward model training, which tries to read this parameter in init.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    logging_first_step: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\"Whether to log and evaluate the first global_step or not.\")\n",
    "        },\n",
    "    )\n",
    "    optim: Optional[str] = field(default=\"adamw_torch\")\n",
    "    train_batch_size: Optional[int] = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": (\"The batch size for training.\")},\n",
    "    )\n",
    "    eval_batch_size: Optional[int] = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": (\"The batch size for eval.\")},\n",
    "    )    \n",
    "    epochs: Optional[int] = field(\n",
    "        default=3, metadata={\"help\": (\"The number of epochs to train for.\")}\n",
    "    )\n",
    "    checkpoint_save_steps: Optional[int] = field(\n",
    "        default=50, metadata={\"help\": (\"The number of steps to save the model.\")}\n",
    "    )\n",
    "\n",
    "    logging_steps: Optional[int] = field(\n",
    "        default=10, metadata={\"help\": (\"The number of steps to log the model.\")}\n",
    "    )\n",
    "\n",
    "    weight_decay: Optional[float] = field(\n",
    "        default=0.01, metadata={\"help\": (\"The weight decay to use.\")}\n",
    "    )\n",
    "\n",
    "    lr: Optional[float] = field(\n",
    "        default=2e-5, metadata={\"help\": (\"The learning rate to use.\")}\n",
    "    )\n",
    "\n",
    "    output_data_dir: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The output data directory.\")},\n",
    "    )\n",
    "\n",
    "    model_dir: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The model directory.\")},\n",
    "    )\n",
    "\n",
    "    model_checkpoint_dir: Optional[str] = field(\n",
    "        default=\"/opt/ml/checkpoints\",\n",
    "        metadata={\"help\": (\"The model checkpoint directory.\")},\n",
    "    )\n",
    "\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=4, metadata={\"help\": (\"The number of gradient accumulation steps.\")}\n",
    "    )\n",
    "\n",
    "    resume_from_checkpoint: Optional[bool] = field(\n",
    "        default=False, metadata={\"help\": (\"Whether to resume from a checkpoint.\")}\n",
    "    )\n",
    "\n",
    "    warmup_ratio: Optional[float] = field(\n",
    "        default=0.1, metadata={\"help\": (\"The warmup ratio.\")}\n",
    "    )\n",
    "    lr_scheduler_type: Optional[str] = field(\n",
    "        default=\"linear\", metadata={\"help\": (\"The learning rate scheduler type.\")}\n",
    "    )\n",
    "    packing: Optional[bool] = field(default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b3179-c90d-4e53-9979-86024545423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_checkpoint(output_dir: str):\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(output_dir):\n",
    "        last_checkpoint = get_last_checkpoint(output_dir)\n",
    "    return last_checkpoint\n",
    "\n",
    "\n",
    "def get_quantization_config(model_args: ModelArguments) -> BitsAndBytesConfig | None:\n",
    "    if model_args.load_in_4bit:\n",
    "        compute_dtype = torch.float16\n",
    "        if model_args.torch_dtype not in {\"auto\", None}:\n",
    "            compute_dtype = getattr(torch, model_args.torch_dtype)\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n",
    "            bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n",
    "            bnb_4bit_quant_storage=model_args.bnb_4bit_quant_storage,\n",
    "        )\n",
    "    elif model_args.load_in_8bit:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "\n",
    "    return quantization_config\n",
    "\n",
    "\n",
    "def get_current_device() -> int:\n",
    "    \"\"\"Get the current device. For GPU we return the local process index to enable multiple GPU training.\"\"\"\n",
    "    return Accelerator().local_process_index if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_kbit_device_map() -> Dict[str, int] | None:\n",
    "    \"\"\"Useful for running inference with quantized models by setting `device_map=get_peft_device_map()`\"\"\"\n",
    "    return {\"\": get_current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "\n",
    "def parse_args() -> Tuple[ModelArguments, DataArguments, SFTConfig]:\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments, SFTConfig))\n",
    "    return parser.parse_args_into_dataclasses()\n",
    "\n",
    "\n",
    "instruction = \"rewrite: \"\n",
    "text_column = \"query\"\n",
    "summary_column = \"alternative\"\n",
    "seed = 12231\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples, tokenizer, max_length: int = 32):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] and examples[summary_column][i]:\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[summary_column][i])\n",
    "\n",
    "    inputs = [instruction + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets, max_length=max_length, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # # padding in the loss.\n",
    "    # if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def create_ds_from_parquet(data_path, tokenizer, max_seq_length):\n",
    "    dataset = (\n",
    "        Dataset.from_parquet(data_path)\n",
    "        .map(\n",
    "            lambda d: formatting_prompts_func(\n",
    "                examples=d, tokenizer=tokenizer, max_length=max_seq_length\n",
    "            ),\n",
    "            batched=True,\n",
    "        )\n",
    "        .shuffle(seed=seed)\n",
    "        .remove_columns([text_column, summary_column])\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_args: ModelArguments,\n",
    "    data_args: DataArguments,\n",
    "    training_args: SFTConfig,\n",
    "    seed: int = 3407,\n",
    "):\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(seed)\n",
    "\n",
    "    ###############\n",
    "    # Setup logging\n",
    "    ###############\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = logging.INFO\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Check for last checkpoint\n",
    "    last_checkpoint = get_checkpoint(training_args.model_checkpoint_dir)\n",
    "    if last_checkpoint is not None and training_args.resume_from_checkpoint:\n",
    "        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name,\n",
    "    )\n",
    "    tokenizer.pad_token = (\n",
    "        tokenizer.unk_token\n",
    "    )  # use unk rather than eos token to prevent endless generation\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "    logger.info(\"*** Load pretrained model ***\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_args.model_name)\n",
    "\n",
    "    # Train the model\n",
    "    # @title Show current memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "    train_ds = create_ds_from_parquet(\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=data_args.train_dataset_path,\n",
    "        max_seq_length=training_args.max_seq_length,\n",
    "    )\n",
    "    test_ds = create_ds_from_parquet(\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=data_args.train_dataset_path,\n",
    "        max_seq_length=training_args.max_seq_length,\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_ds) * training_args.epochs // training_args.train_batch_size\n",
    "\n",
    "    print(train_ds[0])\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        args=Seq2SeqTrainingArguments(\n",
    "            per_device_train_batch_size=training_args.train_batch_size,\n",
    "            per_device_eval_batch_size=training_args.eval_batch_size,\n",
    "            gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "            warmup_steps=int(total_steps * training_args.warmup_ratio),\n",
    "            logging_dir=training_args.model_checkpoint_dir,\n",
    "            num_train_epochs=training_args.epochs,\n",
    "            learning_rate=training_args.lr,\n",
    "            # 'fp16' is set to True if bfloat16 is not supported, which means the model will use 16-bit floating point precision for training if possible.\n",
    "            # fp16=not torch.cuda.is_bf16_supported(),\n",
    "            # 'bf16' is set to True if bfloat16 is supported, which means the model will use bfloat16 precision for training if possible.\n",
    "            # bf16=torch.cuda.is_bf16_supported(),\n",
    "            logging_steps=training_args.logging_steps,\n",
    "            optim=training_args.optim,\n",
    "            weight_decay=training_args.weight_decay,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=seed,\n",
    "            output_dir=training_args.model_checkpoint_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=training_args.checkpoint_save_steps,\n",
    "            restore_callback_states_from_checkpoint=False,\n",
    "            eval_steps=training_args.logging_steps,\n",
    "            eval_strategy=\"steps\",\n",
    "            greater_is_better=False,\n",
    "            save_total_limit=2,\n",
    "            # gradient_checkpointing=True,\n",
    "        ),\n",
    "    )\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "    used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "    print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "    print(\n",
    "        f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    "    )\n",
    "    print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "    trainer.save_model(training_args.model_dir)  # Local saving\n",
    "    tokenizer.save_pretrained(training_args.model_dir)\n",
    "\n",
    "    return trainer.model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464eab0-3128-418a-a8cc-a248396d698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"openai-community/gpt2\"\n",
    "model_name = \"MBZUAI/LaMini-Flan-T5-77M\"\n",
    "model_name = \"MBZUAI/LaMini-T5-61M\"\n",
    "last_name = model_name.split(\"/\")[-1]\n",
    "model_args = ModelArguments(\n",
    "    model_name=model_name,\n",
    "    lora_alpha=64,\n",
    "    lora_r=32,\n",
    ")\n",
    "data_args = DataArguments(\n",
    "    train_dataset_path=\"./train_sample_query_rewrite_nodup_full_sample_clean.parquet\",\n",
    "    test_dataset_path=\"./test_sample_query_rewrite_nodup_full_sample_clean.parquet\"\n",
    ")\n",
    "train_args = SFTConfig(\n",
    "    model_dir=f\"models/{last_name}\",\n",
    "    model_checkpoint_dir=f\"models/{last_name}-checkpoint\",\n",
    "    output_data_dir=f\"models/{last_name}-data\",\n",
    "    train_batch_size=256,\n",
    "    eval_batch_size=512,\n",
    "    epochs=5,\n",
    "    lr=3e-4,\n",
    "    max_seq_length=32,\n",
    "    resume_from_checkpoint=False,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_ratio=0.1,\n",
    "    checkpoint_save_steps=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6de9a-aecb-426d-b8b4-1310724ff615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = main(\n",
    "    model_args=model_args,\n",
    "    training_args=train_args,\n",
    "    data_args=data_args\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
