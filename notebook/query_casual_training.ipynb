{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c09dae-93b1-4832-a89a-9fd912f1d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = field(\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\n",
    "            \"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"\n",
    "        },\n",
    "    )\n",
    "    model_code_revision: str = field(\n",
    "        default=None, metadata={\"help\": \"The branch of the IFT model\"}\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The path to the tokenizer. Useful if you want to use a different tokenizer to the one stored in `model_name_or_path`.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False, metadata={\"help\": \"Trust remote code when loading a model.\"}\n",
    "    )\n",
    "    use_flash_attention_2: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to use flash attention 2. You must install this manually by running `pip install flash-attn --no-build-isolation`\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    use_peft: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": (\"Whether to use PEFT or not for training.\")},\n",
    "    )\n",
    "    lora_r: Optional[int] = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": (\"LoRA R value.\")},\n",
    "    )\n",
    "    lora_alpha: Optional[int] = field(\n",
    "        default=32,\n",
    "        metadata={\"help\": (\"LoRA alpha.\")},\n",
    "    )\n",
    "    lora_dropout: Optional[float] = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": (\"LoRA dropout.\")},\n",
    "    )\n",
    "    lora_target_modules: Optional[List[str]] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"LoRA target modules.\")},\n",
    "    )\n",
    "    lora_modules_to_save: Optional[List[str]] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"Model layers to unfreeze & train\")},\n",
    "    )\n",
    "    load_in_8bit: bool = field(default=False, metadata={\"help\": \"use 8 bit precision\"})\n",
    "    load_in_4bit: bool = field(default=False, metadata={\"help\": \"use 4 bit precision\"})\n",
    "\n",
    "    bnb_4bit_quant_type: Optional[str] = field(\n",
    "        default=\"nf4\", metadata={\"help\": \"precise the quantization type (fp4 or nf4)\"}\n",
    "    )\n",
    "    use_bnb_nested_quant: bool = field(\n",
    "        default=False, metadata={\"help\": \"use nested quantization\"}\n",
    "    )\n",
    "    bnb_4bit_quant_storage: Optional[str] = field(\n",
    "        default=\"uint8\",\n",
    "        metadata={\"help\": \"storage type to pack the quanitzed 4-bit prarams.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.load_in_8bit and self.load_in_4bit:\n",
    "            raise ValueError(\"You can't use 8 bit and 4 bit precision at the same time\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_template: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The chat template to use.\"}\n",
    "    )\n",
    "\n",
    "    text_column: Optional[str] = field(\n",
    "        default=\"text\",\n",
    "        metadata={\n",
    "            \"help\": \"The column name to use for the text in the dataset (only used for continued pretraining).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    truncation_side: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Truncation side to use for the tokenizer.\"}\n",
    "    )\n",
    "    auto_insert_empty_system_msg: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to automatically insert an empty system message as the first message if `system` is mentioned in the chat template.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    train_dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The path to the training dataset.\")},\n",
    "    )\n",
    "    test_dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The path to the training dataset.\")},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig:\n",
    "    \"\"\"\n",
    "    Arguments related to the training process itself. For all parameters, see: https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    Also used for the continued pretraining task.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_kwargs: Optional[Dict[str, Any]] = field(\n",
    "        default=None, metadata={\"help\": \"Dataset kwargs for the SFTTrainer\"}\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=8196,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Used by TRL for reward model training, which tries to read this parameter in init.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    logging_first_step: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\"Whether to log and evaluate the first global_step or not.\")\n",
    "        },\n",
    "    )\n",
    "    optim: Optional[str] = field(default=\"adamw_torch\")\n",
    "    train_batch_size: Optional[int] = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": (\"The batch size for training.\")},\n",
    "    )\n",
    "    epochs: Optional[int] = field(\n",
    "        default=3, metadata={\"help\": (\"The number of epochs to train for.\")}\n",
    "    )\n",
    "    checkpoint_save_steps: Optional[int] = field(\n",
    "        default=50, metadata={\"help\": (\"The number of steps to save the model.\")}\n",
    "    )\n",
    "\n",
    "    logging_steps: Optional[int] = field(\n",
    "        default=10, metadata={\"help\": (\"The number of steps to log the model.\")}\n",
    "    )\n",
    "\n",
    "    weight_decay: Optional[float] = field(\n",
    "        default=0.01, metadata={\"help\": (\"The weight decay to use.\")}\n",
    "    )\n",
    "\n",
    "    lr: Optional[float] = field(\n",
    "        default=2e-5, metadata={\"help\": (\"The learning rate to use.\")}\n",
    "    )\n",
    "\n",
    "    output_data_dir: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The output data directory.\")},\n",
    "    )\n",
    "\n",
    "    model_dir: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The model directory.\")},\n",
    "    )\n",
    "\n",
    "    model_checkpoint_dir: Optional[str] = field(\n",
    "        default=\"/opt/ml/checkpoints\",\n",
    "        metadata={\"help\": (\"The model checkpoint directory.\")},\n",
    "    )\n",
    "\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=4, metadata={\"help\": (\"The number of gradient accumulation steps.\")}\n",
    "    )\n",
    "\n",
    "    resume_from_checkpoint: Optional[bool] = field(\n",
    "        default=False, metadata={\"help\": (\"Whether to resume from a checkpoint.\")}\n",
    "    )\n",
    "\n",
    "    warmup_ratio: Optional[float] = field(\n",
    "        default=0.1, metadata={\"help\": (\"The warmup ratio.\")}\n",
    "    )\n",
    "    lr_scheduler_type: Optional[str] = field(\n",
    "        default=\"linear\", metadata={\"help\": (\"The learning rate scheduler type.\")}\n",
    "    )\n",
    "    packing: Optional[bool] = field(default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd9fe7-e6a8-4482-9464-f2f21ed2c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    HfArgumentParser,\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_checkpoint(output_dir: str):\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(output_dir):\n",
    "        last_checkpoint = get_last_checkpoint(output_dir)\n",
    "    return last_checkpoint\n",
    "\n",
    "\n",
    "def get_quantization_config(model_args: ModelArguments) -> BitsAndBytesConfig | None:\n",
    "    if model_args.load_in_4bit:\n",
    "        compute_dtype = torch.float16\n",
    "        if model_args.torch_dtype not in {\"auto\", None}:\n",
    "            compute_dtype = getattr(torch, model_args.torch_dtype)\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n",
    "            bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n",
    "            bnb_4bit_quant_storage=model_args.bnb_4bit_quant_storage,\n",
    "        )\n",
    "    elif model_args.load_in_8bit:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "\n",
    "    return quantization_config\n",
    "\n",
    "\n",
    "def get_current_device() -> int:\n",
    "    \"\"\"Get the current device. For GPU we return the local process index to enable multiple GPU training.\"\"\"\n",
    "    return Accelerator().local_process_index if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_kbit_device_map() -> Dict[str, int] | None:\n",
    "    \"\"\"Useful for running inference with quantized models by setting `device_map=get_peft_device_map()`\"\"\"\n",
    "    return {\"\": get_current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "\n",
    "def parse_args() -> Tuple[ModelArguments, DataArguments, SFTConfig]:\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments, SFTConfig))\n",
    "    return parser.parse_args_into_dataclasses()\n",
    "\n",
    "\n",
    "default_prompt = \"\"\"{}: {}\\n {}\"\"\"\n",
    "# instruction = \"Instruct: Generate adequate search engine query to obtain requested information.\"\n",
    "instruction = \"Rewrite this search query\"\n",
    "\n",
    "def formatting_prompts_func(example, eos_token):\n",
    "\n",
    "    input = example.get(\"query\", '')\n",
    "    output = example.get('alternative', '')\n",
    "    text = default_prompt.format(instruction, input, output) + eos_token\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "def create_ds_from_parquet(data_path, tokenizer, max_seq_length):\n",
    "    dataset = (\n",
    "        Dataset.from_parquet(data_path)\n",
    "        .map(\n",
    "            lambda d: formatting_prompts_func(\n",
    "                example=d, eos_token=tokenizer.eos_token\n",
    "            ),\n",
    "            batched=False,\n",
    "        )\n",
    "        .shuffle(seed=411)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main(model_args: ModelArguments, data_args: DataArguments, training_args: SFTConfig, seed: int = 3407):\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(seed)\n",
    "\n",
    "    ###############\n",
    "    # Setup logging\n",
    "    ###############\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = logging.INFO\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "    # Check for last checkpoint\n",
    "    last_checkpoint = get_checkpoint(training_args.model_checkpoint_dir)\n",
    "    if last_checkpoint is not None and training_args.resume_from_checkpoint:\n",
    "        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name,\n",
    "    )\n",
    "    tokenizer.pad_token = (\n",
    "        tokenizer.unk_token\n",
    "    )  # use unk rather than eos token to prevent endless generation\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "    logger.info(\"*** Load pretrained model ***\")\n",
    "    torch_dtype = (\n",
    "        torch.bfloat16 if not torch.cuda.is_bf16_supported() else torch.float16\n",
    "    )\n",
    "    quantization_config = get_quantization_config(model_args)\n",
    "\n",
    "    model_kwargs = dict(\n",
    "        trust_remote_code=True,\n",
    "        # attn_implementation=\"flash_attention_2\",\n",
    "        # torch_dtype=torch_dtype,\n",
    "        device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=model_args.lora_r,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules=model_args.lora_target_modules,\n",
    "        lora_alpha=model_args.lora_alpha,\n",
    "        lora_dropout=model_args.lora_dropout,  # Supports any, but = 0 is optimized\n",
    "        bias=\"none\",  # Supports any, but = \"none\" is optimized)\n",
    "        use_rslora=False,  # We support rank stabilized LoRA\n",
    "        # loftq_config=None,  # And LoftQ\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_args.model_name, **model_kwargs)\n",
    "    # model = get_peft_model(model, peft_config)\n",
    "    # model.print_trainable_parameters()\n",
    "\n",
    "    # Train the model\n",
    "    # @title Show current memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "    train_ds = create_ds_from_parquet(\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=data_args.train_dataset_path,\n",
    "        max_seq_length=training_args.max_seq_length,\n",
    "    )\n",
    "    test_ds = create_ds_from_parquet(\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=data_args.train_dataset_path,\n",
    "        max_seq_length=training_args.max_seq_length,\n",
    "    )\n",
    "    print(train_ds[0])\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=training_args.max_seq_length,\n",
    "        dataset_num_proc=1,\n",
    "        packing=training_args.packing,  # Can make training 5x faster for short sequences.\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=training_args.train_batch_size,\n",
    "            gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "            warmup_steps=5,\n",
    "            logging_dir=training_args.output_data_dir,\n",
    "            num_train_epochs=training_args.epochs,\n",
    "            learning_rate=training_args.lr,\n",
    "            # 'fp16' is set to True if bfloat16 is not supported, which means the model will use 16-bit floating point precision for training if possible.\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            # 'bf16' is set to True if bfloat16 is supported, which means the model will use bfloat16 precision for training if possible.\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            logging_steps=training_args.logging_steps,\n",
    "            optim=training_args.optim,\n",
    "            weight_decay=training_args.weight_decay,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=seed,\n",
    "            output_dir=training_args.model_dir,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=training_args.checkpoint_save_steps,\n",
    "            restore_callback_states_from_checkpoint=False,\n",
    "            # gradient_checkpointing=True,\n",
    "        ),\n",
    "    )\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "    used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "    print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "    print(\n",
    "        f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    "    )\n",
    "    print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "    trainer.save_model(training_args.output_data_dir)  # Local saving\n",
    "    tokenizer.save_pretrained(training_args.output_data_dir)\n",
    "\n",
    "    return trainer.model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3464eab0-3128-418a-a8cc-a248396d698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "# model_name = \"jtatman/gpt2-open-instruct-v1-Anthropic-hh-rlhf\"\n",
    "# model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "# last_name = model_name.split(\"/\")[-1]\n",
    "model_args = ModelArguments(\n",
    "    model_name=model_name,\n",
    "    lora_alpha=64,\n",
    "    lora_r=32,\n",
    ")\n",
    "data_args = DataArguments(\n",
    "    train_dataset_path=\"./sample_query_rewrite_nodup_full.parquet\"\n",
    ")\n",
    "train_args = SFTConfig(\n",
    "    model_dir=f\"models/{last_name}\",\n",
    "    model_checkpoint_dir=f\"models/{last_name}-checkpoint\",\n",
    "    output_data_dir=f\"models/{last_name}-data\",\n",
    "    train_batch_size=64,\n",
    "    packing=True,\n",
    "    epochs=5,\n",
    "    lr=1e-4,\n",
    "    max_seq_length=32,\n",
    "    resume_from_checkpoint=False,\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=1,d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0f6de9a-aecb-426d-b8b4-1310724ff615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:679] 2024-11-08 09:18:16,784 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-08 09:18:16,785 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"openai-community/gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-08 09:18:16,787 >> loading file vocab.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-08 09:18:16,787 >> loading file merges.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-08 09:18:16,788 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-08 09:18:16,788 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-08 09:18:16,789 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-08 09:18:16,790 >> loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:679] 2024-11-08 09:18:16,791 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-08 09:18:16,792 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"openai-community/gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:16 - INFO - __main__ - *** Load pretrained model ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:679] 2024-11-08 09:18:17,003 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-08 09:18:17,005 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"openai-community/gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3937] 2024-11-08 09:18:17,008 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "[INFO|modeling_utils.py:1670] 2024-11-08 09:18:17,015 >> Instantiating GPT2LMHeadModel model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1096] 2024-11-08 09:18:17,018 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4800] 2024-11-08 09:18:17,477 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-11-08 09:18:17,478 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at openai-community/gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:1051] 2024-11-08 09:18:17,561 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-11-08 09:18:17,562 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-b0d48c55225bc05a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 125,029,632 || trainable%: 0.4717\n",
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "3.295 GB of memory reserved.\n",
      "2024-11-08 09:18:17 - INFO - datasets.builder - Using custom data configuration default-b0d48c55225bc05a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/ubuntu/.local/lib/python3.10/site-packages/datasets/packaged_modules/parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.local/lib/python3.10/site-packages/datasets/packaged_modules/parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.builder - Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faba16c8bd24f929c308373c2acde58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6/cache-f7d9210cb25bca3e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6/cache-f7d9210cb25bca3e.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching indices mapping at /home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6/cache-360b941713eadb4e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/ubuntu/.cache/huggingface/datasets/parquet/default-b0d48c55225bc05a/0.0.0/9d41700293b5cf3c3cee6167e8c49e37598331b6466506aecb40a8c11b6aa9f6/cache-360b941713eadb4e.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:2147] 2024-11-08 09:18:17,767 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1822] 2024-11-08 09:18:17,787 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_num_proc, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[INFO|training_args.py:2147] 2024-11-08 09:18:17,791 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is a normal cholesterol score', 'alternative': 'healthy cholesterol levels', 'semantic_score': 0.7151662707328796, '__index_level_0__': 919, 'text': 'Rewrite this search query: what is a normal cholesterol score\\n healthy cholesterol levels<|endoftext|>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:297: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-791ab15b4db867b0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.builder - Using custom data configuration default-791ab15b4db867b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/ubuntu/.local/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.local/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset generator (/home/ubuntu/.cache/huggingface/datasets/generator/default-791ab15b4db867b0/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.builder - Generating dataset generator (/home/ubuntu/.cache/huggingface/datasets/generator/default-791ab15b4db867b0/0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/ubuntu/.cache/huggingface/datasets/generator/default-791ab15b4db867b0/0.0.0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.builder - Downloading and preparing dataset generator/default to /home/ubuntu/.cache/huggingface/datasets/generator/default-791ab15b4db867b0/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:17 - INFO - datasets.builder - Generating train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7519a58754e41258945ed13d7f86ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:18 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/generator/default-791ab15b4db867b0/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 09:18:18 - INFO - datasets.builder - Dataset generator downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/generator/default-791ab15b4db867b0/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "[INFO|trainer.py:698] 2024-11-08 09:18:18,238 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2313] 2024-11-08 09:18:18,546 >> ***** Running training *****\n",
      "[INFO|trainer.py:2314] 2024-11-08 09:18:18,547 >>   Num examples = 1,301\n",
      "[INFO|trainer.py:2315] 2024-11-08 09:18:18,548 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2316] 2024-11-08 09:18:18,549 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:2319] 2024-11-08 09:18:18,549 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2320] 2024-11-08 09:18:18,550 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2321] 2024-11-08 09:18:18,551 >>   Total optimization steps = 105\n",
      "[INFO|trainer.py:2322] 2024-11-08 09:18:18,553 >>   Number of trainable parameters = 589,824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.820400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.559100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3801] 2024-11-08 09:18:42,169 >> Saving model checkpoint to models/gpt2/checkpoint-50\n",
      "[INFO|configuration_utils.py:679] 2024-11-08 09:18:42,345 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-08 09:18:42,347 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-08 09:18:42,354 >> tokenizer config file saved in models/gpt2/checkpoint-50/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-08 09:18:42,357 >> Special tokens file saved in models/gpt2/checkpoint-50/special_tokens_map.json\n",
      "[INFO|trainer.py:3801] 2024-11-08 09:19:06,200 >> Saving model checkpoint to models/gpt2/checkpoint-100\n",
      "[INFO|configuration_utils.py:679] 2024-11-08 09:19:06,393 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-08 09:19:06,394 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-08 09:19:06,402 >> tokenizer config file saved in models/gpt2/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-08 09:19:06,405 >> Special tokens file saved in models/gpt2/checkpoint-100/special_tokens_map.json\n",
      "[INFO|trainer.py:3801] 2024-11-08 09:19:08,614 >> Saving model checkpoint to models/gpt2/checkpoint-105\n",
      "[INFO|configuration_utils.py:679] 2024-11-08 09:19:08,803 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-08 09:19:08,804 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-08 09:19:08,811 >> tokenizer config file saved in models/gpt2/checkpoint-105/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-08 09:19:08,812 >> Special tokens file saved in models/gpt2/checkpoint-105/special_tokens_map.json\n",
      "[INFO|trainer.py:2584] 2024-11-08 09:19:08,871 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:3801] 2024-11-08 09:19:08,875 >> Saving model checkpoint to models/gpt2-data\n",
      "[INFO|configuration_utils.py:679] 2024-11-08 09:19:09,056 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-08 09:19:09,058 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-08 09:19:09,066 >> tokenizer config file saved in models/gpt2-data/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-08 09:19:09,069 >> Special tokens file saved in models/gpt2-data/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.3181 seconds used for training.\n",
      "0.84 minutes used for training.\n",
      "Peak reserved memory = 3.67 GB.\n",
      "Peak reserved memory for training = 0.375 GB.\n",
      "Peak reserved memory % of max memory = 24.897 %.\n",
      "Peak reserved memory for training % of max memory = 2.544 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2646] 2024-11-08 09:19:09,132 >> tokenizer config file saved in models/gpt2-data/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-08 09:19:09,133 >> Special tokens file saved in models/gpt2-data/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = main(\n",
    "    model_args=model_args,\n",
    "    training_args=train_args,\n",
    "    data_args=data_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "17b20761-a0be-449f-984e-66d2789b100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|utils.py:1831] 2024-11-08 09:21:28,112 >> Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  ['Rewrite this search query: Who lived longer, Nikola Tesla or Milutin Milankovic?\\n \\xa0 \\xa0Who lived longer Nikola Tesla or Milutin']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [default_prompt.format(instruction, \"Who lived longer, Nikola Tesla or Milutin Milankovic?\", \"\")],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=12, use_cache=False, temperature=0.7, do_sample=True, top_p=0.95\n",
    ")\n",
    "print(\"Answer: \", tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b05cf-a9f6-441f-bcf2-b0e6153026ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a829698-b78c-4053-8573-9e3cb5505967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db454f-34da-4128-b6d4-1ae0c11cf360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80feba6e-22b7-4c53-8b1c-95f63bfbe585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
